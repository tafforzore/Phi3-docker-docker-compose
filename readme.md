
# ğŸš€ DÃ©ploiement de l'API Ollama avec Docker â€“ ModÃ¨le Phi-3

Bienvenue dans la documentation de dÃ©ploiement et dâ€™utilisation de lâ€™API **Ollama**, conÃ§ue pour interagir avec des modÃ¨les LLM tels que **Phi-3**, **LLaMA2**, **Mistral**, etc. Ce guide vous permet de lancer rapidement une instance locale avec Docker, et d'exploiter les routes REST pour la gÃ©nÃ©ration de texte, les conversations type ChatGPT et le contrÃ´le serveur.

> ğŸ”— DÃ©pÃ´t GitHub : [https://github.com/tafforzore/Phi3-docker-docker-compose](https://github.com/tafforzore/Phi3-docker-docker-compose)  
> ğŸ“ Serveur par dÃ©faut : `http://localhost:11434`

---

## ğŸ³ DÃ©ploiement avec Docker

```bash
git clone https://github.com/tafforzore/Phi3-docker-docker-compose.git
cd Phi3-docker-docker-compose
docker compose up -d
```

---

## ğŸ“˜ API REST â€“ Routes disponibles

### ğŸ” `POST /api/generate` â€“ GÃ©nÃ©ration de texte

CrÃ©e une complÃ©tion Ã  partir dâ€™un prompt donnÃ©.

#### Payload

```json
{
  "model": "phi3",
  "prompt": "Explique la gravitation universelle.",
  "stream": false,
  "system": "Tu es un professeur de physique.",
  "template": "",
  "options": {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.1,
    "seed": 42,
    "num_predict": 200
  }
}
```

#### Exemple de rÃ©ponse

```json
{
  "model": "phi3",
  "created_at": "2025-07-17T10:21:42.437Z",
  "response": "La gravitation est la force qui attire deux objets possÃ©dant une masse...",
  "done": true
}
```

---

### ğŸ¤– `POST /api/chat` â€“ Interaction type ChatGPT

Dialogue avec un modÃ¨le LLM en conservant un historique.

#### Payload

```json
{
  "model": "phi3",
  "messages": [
    {"role": "system", "content": "Tu es un assistant mÃ©dical."},
    {"role": "user", "content": "Quels sont les symptÃ´mes du paludisme ?"}
  ]
}
```

#### Exemple de rÃ©ponse

```json
{
  "message": {
    "role": "assistant",
    "content": "Les symptÃ´mes du paludisme sont : fiÃ¨vre, frissons, maux de tÃªte..."
  },
  "done": true
}
```

---

### â™¥ï¸ `GET /` â€“ Ping du serveur

Pour tester si le serveur est bien lancÃ©.

```json
{"status": "ok"}
```

---

## âš™ï¸ DÃ©tails techniques

- Serveur sur `http://localhost:11434`
- Format : `application/json`
- ModÃ¨les supportÃ©s : `phi3`, `llama2`, `mistral`, etc.
- PossibilitÃ© dâ€™activer le **streaming** (`stream: true`)
- Pas dâ€™authentification par dÃ©faut âœ âš ï¸ Utiliser un proxy ou un VPN

---

## ğŸ“š Ressources complÃ©mentaires

- ğŸŒ [Site officiel dâ€™Ollama](https://ollama.com)
- ğŸ“¦ [BibliothÃ¨que de modÃ¨les](https://ollama.com/library)
- ğŸ’» [Code source client GitHub](https://github.com/ollama/ollama)

---

## âœ¨ Bonus : Swagger ou Postman

Tu peux me demander une version OpenAPI 3.0 (Swagger JSON) pour une intÃ©gration rapide dans **Swagger UI** ou **Postman**.

---

## ğŸ” Optimisation SEO

Ce projet rÃ©pond Ã  la recherche :  
**"DÃ©ploiement de Phi3 avec Docker"**  
**"API Ollama modÃ¨le Phi-3"**  
**"Exemple de chat LLM local avec Docker"**

---

ğŸ‘¨â€ğŸ’» *Maintenu par [@tafforzore](https://github.com/tafforzore)*  
ğŸ› ï¸ *AmÃ©liorations et contributions bienvenues !*
